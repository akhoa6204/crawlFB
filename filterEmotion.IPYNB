{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data.dataset import Dataset, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data_train_1.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df = df[['content', 'label']]\n",
    "\n",
    "labels_map = {\n",
    "    \"POS\": 0,\n",
    "    \"NEU\": 1,\n",
    "    \"NEG\": 2\n",
    "}\n",
    "\n",
    "df['label'] = df['label'].map(labels_map)\n",
    "df = df.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  \n",
    "        u\"\\U0001F300-\\U0001F5FF\"  \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F700-\\U0001F77F\"  \n",
    "        u\"\\U0001F780-\\U0001F7FF\"  \n",
    "        u\"\\U0001F800-\\U0001F8FF\"  \n",
    "        u\"\\U0001F900-\\U0001F9FF\"  \n",
    "        u\"\\U0001FA00-\\U0001FA6F\"  \n",
    "        u\"\\U0001FA70-\\U0001FAFF\"  \n",
    "        u\"\\U00002702-\\U000027B0\"  \n",
    "        u\"\\U000024C2-\\U0001F251\" \n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  \n",
    "    text = re.sub(r'\\d+', ' <num> ', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text).strip() \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2604</th>\n",
       "      <td>Vẫn tặng bạn 5* vì nhiệt tình</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16959</th>\n",
       "      <td>Dù có 5k nhưg mất uy tín</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19018</th>\n",
       "      <td>Hàng đẹp dã man chị trang ạ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20374</th>\n",
       "      <td>Và con nào ăn thì con đấy chết</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11124</th>\n",
       "      <td>Chất lượng tốt có nhiều quà tặng kèm</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9676</th>\n",
       "      <td>Có giảm nhưng uống vào người rất mệt và buồn nôn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>Đóng gói sản phẩm rất đẹp và chắc chắn Shop ph...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6249</th>\n",
       "      <td>Đây chỉ là góp ý và không nỡ để shop bị rate t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>Chất vải và kiểu áo đều ok, dễ mặc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695</th>\n",
       "      <td>Chất vải quá tồi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  label\n",
       "2604                       Vẫn tặng bạn 5* vì nhiệt tình      0\n",
       "16959                           Dù có 5k nhưg mất uy tín      1\n",
       "19018                        Hàng đẹp dã man chị trang ạ      0\n",
       "20374                     Và con nào ăn thì con đấy chết      2\n",
       "11124               Chất lượng tốt có nhiều quà tặng kèm      0\n",
       "...                                                  ...    ...\n",
       "9676    Có giảm nhưng uống vào người rất mệt và buồn nôn      1\n",
       "622    Đóng gói sản phẩm rất đẹp và chắc chắn Shop ph...      0\n",
       "6249   Đây chỉ là góp ý và không nỡ để shop bị rate t...      1\n",
       "926                   Chất vải và kiểu áo đều ok, dễ mặc      0\n",
       "9695                                    Chất vải quá tồi      2\n",
       "\n",
       "[15000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df.sample(n=15000, random_state=42)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['content'] = df_test['content'].apply(remove_emoji)  \n",
    "df_test['content'] = df_test['content'].apply(clean_text)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df_test, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "checkpoint = 'distilbert-base-multilingual-cased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(checkpoint)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"content\"], truncation=True, padding = True, max_length = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12000/12000 [00:01<00:00, 6501.04 examples/s]\n",
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 4352.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.remove_columns(column_names='content')\n",
    "train_dataset = train_dataset.remove_columns(column_names='__index_level_0__')\n",
    "train_dataset = train_dataset.remove_columns(column_names='attention_mask')\n",
    "val_dataset = val_dataset.remove_columns(column_names='content')\n",
    "val_dataset = val_dataset.remove_columns(column_names='__index_level_0__')\n",
    "val_dataset = val_dataset.remove_columns(column_names='attention_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids'],\n",
       "    num_rows: 12000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=10, collate_fn=data_collator)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=10, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m path1\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:/kpw/crawlFB/Data-facebook/text\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m np\u001b[38;5;241m.\u001b[39msave(path1, \u001b[43marr\u001b[49m)    \u001b[38;5;66;03m# .npy extension is added if not given\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(path1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'arr' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "path1='D:/kpw/crawlFB/Data-facebook/text'+ \".npy\"\n",
    "np.save(path1, arr)    # .npy extension is added if not given\n",
    "print(path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch.keys())\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, max_length):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 64)\n",
    "        self.fc2 = nn.Linear(64, 8)\n",
    "        self.fc3 = nn.Linear(8, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "#         self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(2, batch_size, hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(2, batch_size, hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        out = self.relu(self.fc1(out[:, -1, :]))\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.dropout(out)\n",
    "#         out = self.softmax(self.fc3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 119547\n",
      "Embedding dimension: 128\n",
      "Hidden size: 64\n",
      "Output size: 3\n",
      "Max length: 128\n",
      "Number of epochs: 10\n"
     ]
    }
   ],
   "source": [
    "# HYPER PARAMS\n",
    "\n",
    "vocab_size = tokenizer.vocab_size\n",
    "embedding_dim = 128\n",
    "hidden_size = 64\n",
    "output_size = 3\n",
    "max_length = 128\n",
    "num_epochs = 10\n",
    "device = device\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "print(f\"Hidden size: {hidden_size}\")\n",
    "print(f\"Output size: {output_size}\")\n",
    "print(f\"Max length: {max_length}\")\n",
    "print(f\"Number of epochs: {num_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=10, save_best_model_path=None, save_last_model_path=None):\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state_dict = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            inputs, labels = batch['input_ids'], batch['labels'] \n",
    "            \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_train_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "        \n",
    "        # Calculate average train loss and accuracy for the epoch\n",
    "        train_loss = running_train_loss / len(train_loader)\n",
    "        train_accuracy = correct_predictions / total_predictions\n",
    "        \n",
    "        # Validation loop\n",
    "        val_loss, val_accuracy = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_model_state_dict = model.state_dict()\n",
    "            if save_best_model_path:\n",
    "                torch.save(model.state_dict(), save_best_model_path)\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "    # Save last model\n",
    "    if save_last_model_path:\n",
    "        torch.save(model.state_dict(), save_last_model_path)\n",
    "        \n",
    "    print(\"Training finished.\")\n",
    "    \n",
    "    return best_model_state_dict, best_val_accuracy\n",
    "\n",
    "# Evaluation function (for validation and testing)\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    running_val_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            inputs, labels = batch['input_ids'], batch['labels']\n",
    "            \n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "#             print(predicted)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "    \n",
    "    val_loss = running_val_loss / len(loader)\n",
    "    val_accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 1.4773, Val Loss: 1.2081, Train Accuracy: 0.5218, Val Accuracy: 0.6367\n",
      "Epoch [2/20], Train Loss: 1.4625, Val Loss: 1.2397, Train Accuracy: 0.5236, Val Accuracy: 0.6367\n",
      "Epoch [3/20], Train Loss: 1.4530, Val Loss: 1.2373, Train Accuracy: 0.5275, Val Accuracy: 0.6367\n",
      "Epoch [4/20], Train Loss: 1.4913, Val Loss: 1.2363, Train Accuracy: 0.5112, Val Accuracy: 0.6367\n",
      "Epoch [5/20], Train Loss: 1.4727, Val Loss: 1.2401, Train Accuracy: 0.5148, Val Accuracy: 0.6367\n",
      "Epoch [6/20], Train Loss: 1.4590, Val Loss: 1.0487, Train Accuracy: 0.5132, Val Accuracy: 0.6367\n",
      "Epoch [7/20], Train Loss: 1.2743, Val Loss: 0.8753, Train Accuracy: 0.5845, Val Accuracy: 0.7497\n",
      "Epoch [8/20], Train Loss: 1.1955, Val Loss: 0.8041, Train Accuracy: 0.6335, Val Accuracy: 0.7687\n",
      "Epoch [9/20], Train Loss: 1.1642, Val Loss: 0.8199, Train Accuracy: 0.6613, Val Accuracy: 0.7737\n",
      "Epoch [10/20], Train Loss: 1.1239, Val Loss: 0.8009, Train Accuracy: 0.6723, Val Accuracy: 0.7807\n",
      "Epoch [11/20], Train Loss: 1.1148, Val Loss: 0.8206, Train Accuracy: 0.6879, Val Accuracy: 0.7700\n",
      "Epoch [12/20], Train Loss: 1.0773, Val Loss: 0.7834, Train Accuracy: 0.7100, Val Accuracy: 0.7787\n",
      "Epoch [13/20], Train Loss: 1.0604, Val Loss: 0.7980, Train Accuracy: 0.7332, Val Accuracy: 0.7747\n",
      "Epoch [14/20], Train Loss: 1.0490, Val Loss: 0.8226, Train Accuracy: 0.7558, Val Accuracy: 0.7700\n",
      "Epoch [15/20], Train Loss: 1.0074, Val Loss: 0.8046, Train Accuracy: 0.7712, Val Accuracy: 0.7637\n",
      "Epoch [16/20], Train Loss: 1.0114, Val Loss: 0.8223, Train Accuracy: 0.7842, Val Accuracy: 0.7660\n",
      "Epoch [17/20], Train Loss: 0.9910, Val Loss: 0.8146, Train Accuracy: 0.7939, Val Accuracy: 0.7650\n",
      "Epoch [18/20], Train Loss: 0.9790, Val Loss: 0.8164, Train Accuracy: 0.7966, Val Accuracy: 0.7603\n",
      "Epoch [19/20], Train Loss: 0.9817, Val Loss: 0.8240, Train Accuracy: 0.7987, Val Accuracy: 0.7603\n",
      "Epoch [20/20], Train Loss: 0.9619, Val Loss: 0.8280, Train Accuracy: 0.8137, Val Accuracy: 0.7617\n",
      "Training finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('embedding.weight',\n",
       "               tensor([[ 0.3264, -0.3869, -0.8010,  ..., -0.1576,  0.0389,  0.7377],\n",
       "                       [-0.0929, -1.5505, -1.6151,  ...,  0.6304, -0.9454, -0.0206],\n",
       "                       [ 0.5943,  1.6090, -1.7659,  ..., -0.2266,  0.3290, -1.3098],\n",
       "                       ...,\n",
       "                       [-0.1250, -1.3952, -0.1992,  ...,  0.7997,  0.2099, -1.0816],\n",
       "                       [ 0.8842, -0.3242,  0.2966,  ..., -0.0655, -1.1030, -1.3702],\n",
       "                       [ 0.7403,  0.0351,  0.3931,  ...,  0.9611, -0.3462,  0.2467]])),\n",
       "              ('lstm.weight_ih_l0',\n",
       "               tensor([[ 0.2365, -0.2981, -0.0142,  ...,  0.1053, -0.1460, -0.0732],\n",
       "                       [ 0.3084, -0.0689,  0.1263,  ..., -0.1899, -0.0272,  0.0570],\n",
       "                       [ 0.1843,  0.0373, -0.0904,  ...,  0.1602,  0.1119,  0.0270],\n",
       "                       ...,\n",
       "                       [ 0.0119, -0.4425,  0.2676,  ...,  0.2882, -0.2328,  0.0585],\n",
       "                       [-0.3925, -0.2602, -0.0969,  ..., -0.0757,  0.2237,  0.2198],\n",
       "                       [ 0.0742,  0.4177, -0.1710,  ...,  0.1055,  0.0910,  0.0027]])),\n",
       "              ('lstm.weight_hh_l0',\n",
       "               tensor([[ 0.1105,  0.2965, -0.0466,  ...,  0.0655, -0.3318, -0.4137],\n",
       "                       [ 0.0884,  0.2590,  0.4140,  ...,  0.1959, -0.0769, -0.1892],\n",
       "                       [-0.0217, -0.2225, -0.1427,  ...,  0.1953,  0.1957, -0.0770],\n",
       "                       ...,\n",
       "                       [ 0.0081, -0.0264, -0.1731,  ...,  0.1844,  0.0022,  0.1455],\n",
       "                       [-0.1189, -0.1507,  0.3116,  ...,  0.0681,  0.0588,  0.0510],\n",
       "                       [-0.1253, -0.0150, -0.0863,  ..., -0.1510, -0.1908, -0.0205]])),\n",
       "              ('lstm.bias_ih_l0',\n",
       "               tensor([ 0.0273,  0.2421,  0.1266,  0.4568, -0.0243,  0.0491,  0.1233,  0.1835,\n",
       "                        0.1460, -0.2039,  0.1906, -0.0852, -0.0422,  0.0883, -0.0370, -0.1095,\n",
       "                        0.0290, -0.1565,  0.0228, -0.0996, -0.0959, -0.0350,  0.0466, -0.0439,\n",
       "                       -0.1400, -0.0418, -0.1029, -0.1085,  0.0331,  0.2473, -0.0548, -0.1528,\n",
       "                        0.0073,  0.3278, -0.0432, -0.1744,  0.0480,  0.1617, -0.0731, -0.0986,\n",
       "                        0.4288, -0.0396,  0.0356, -0.0134,  0.0440,  0.0315,  0.1114,  0.1213,\n",
       "                        0.0120,  0.0562,  0.1242,  0.0585,  0.0156,  0.0258, -0.1661, -0.0928,\n",
       "                       -0.0407,  0.2863,  0.0783,  0.2089,  0.0205,  0.1220, -0.1566, -0.0340,\n",
       "                        0.2288,  0.1947,  0.1690,  0.2991,  0.5135,  0.1856,  0.2537,  0.1824,\n",
       "                        0.0078,  0.2864,  0.3015,  0.1604,  0.0968,  0.4512,  0.3561,  0.1683,\n",
       "                        0.0178,  0.1363,  0.3151,  0.1147,  0.3074,  0.1913,  0.1014, -0.0298,\n",
       "                       -0.1372,  0.0875,  0.0520,  0.2463,  0.4594,  0.4434,  0.1298,  0.2742,\n",
       "                       -0.0307,  0.3295,  0.0715,  0.0226, -0.1041,  0.5746,  0.2211,  0.2338,\n",
       "                        0.2352,  0.3906,  0.1962,  0.2174,  0.4912,  0.1627,  0.3672,  0.3572,\n",
       "                        0.1016,  0.2880,  0.4173,  0.1675,  0.3838,  0.4005, -0.0318, -0.0115,\n",
       "                        0.0047,  0.0063,  0.3590,  0.4293,  0.1558,  0.4546,  0.3106,  0.2517,\n",
       "                       -0.1576, -0.1388, -0.1380,  0.0249,  0.0260, -0.0928,  0.1845, -0.0336,\n",
       "                       -0.1174,  0.0825, -0.0713, -0.1687, -0.1601, -0.0318,  0.2090,  0.2242,\n",
       "                        0.0905,  0.2086,  0.0757, -0.1117, -0.1745,  0.0487,  0.3035,  0.1628,\n",
       "                        0.2094, -0.0042,  0.0753,  0.0726,  0.0527, -0.0709, -0.0019,  0.2817,\n",
       "                        0.1231, -0.2545,  0.1380, -0.0294,  0.0158, -0.0979,  0.2166,  0.0164,\n",
       "                       -0.0524, -0.0522, -0.2123, -0.0273, -0.1206,  0.0016, -0.1538, -0.0421,\n",
       "                       -0.0242, -0.0669, -0.2898,  0.0640, -0.2145, -0.1114, -0.1987, -0.0091,\n",
       "                        0.2019,  0.1085,  0.3097,  0.0567,  0.0788, -0.1267,  0.1884, -0.1947,\n",
       "                       -0.1151,  0.1635,  0.2103, -0.0859, -0.0705, -0.0452,  0.3399,  0.0173,\n",
       "                       -0.0056, -0.0454,  0.1862,  0.0691, -0.0514,  0.0018,  0.1152, -0.0727,\n",
       "                        0.1921, -0.0033, -0.0135, -0.0754, -0.0876,  0.0990,  0.0720,  0.0089,\n",
       "                        0.2374,  0.2829,  0.0950, -0.0151, -0.0987,  0.1478, -0.0193,  0.0713,\n",
       "                        0.2004,  0.0934, -0.1963, -0.1031, -0.0371,  0.1003, -0.0034, -0.2112,\n",
       "                        0.2095,  0.1453,  0.1665, -0.1148, -0.0319, -0.0802,  0.1661,  0.0117,\n",
       "                       -0.0174,  0.0586,  0.2020,  0.0209,  0.1535, -0.0433,  0.1962,  0.0231,\n",
       "                       -0.2248, -0.0316,  0.2657,  0.2069,  0.1888,  0.0207,  0.0939, -0.1179])),\n",
       "              ('lstm.bias_hh_l0',\n",
       "               tensor([-0.0136,  0.1686,  0.0574,  0.3439,  0.1480,  0.1719,  0.1443,  0.1285,\n",
       "                        0.1207, -0.1933,  0.1014, -0.1748,  0.0680,  0.1739,  0.0838, -0.1842,\n",
       "                       -0.0778, -0.1708,  0.1808, -0.2336, -0.2421, -0.1012,  0.2463,  0.0090,\n",
       "                       -0.0051, -0.0603, -0.0084, -0.0318,  0.0437,  0.2256, -0.0437,  0.0376,\n",
       "                       -0.0505,  0.0904, -0.1043, -0.0130,  0.0439,  0.1736,  0.1178, -0.0510,\n",
       "                        0.3655, -0.0823, -0.0326,  0.0078, -0.1273,  0.0096,  0.0478,  0.2063,\n",
       "                        0.0017,  0.0713,  0.2110,  0.2222,  0.1180, -0.0484, -0.1820, -0.1737,\n",
       "                        0.0626,  0.1501,  0.2314,  0.2670,  0.1790,  0.1665, -0.0341, -0.1644,\n",
       "                        0.2315,  0.2811,  0.1275,  0.1678,  0.4011,  0.0729,  0.2491,  0.2981,\n",
       "                        0.2031,  0.4016,  0.4745,  0.2488,  0.1710,  0.4707,  0.2306,  0.2098,\n",
       "                        0.0773,  0.3054,  0.5190,  0.1514,  0.0669,  0.0603,  0.2118,  0.0963,\n",
       "                       -0.1034,  0.0267, -0.0673,  0.2610,  0.4718,  0.2419,  0.1725,  0.2109,\n",
       "                        0.0843,  0.1691, -0.0031,  0.0015, -0.0696,  0.5733,  0.4145,  0.1626,\n",
       "                        0.1809,  0.5201,  0.1225,  0.4152,  0.3216,  0.1854,  0.2862,  0.4240,\n",
       "                        0.0496,  0.1416,  0.4205,  0.0951,  0.3529,  0.4601,  0.0925, -0.0085,\n",
       "                       -0.0584,  0.1408,  0.1507,  0.3754, -0.0249,  0.3967,  0.2752,  0.0382,\n",
       "                       -0.1431,  0.0476, -0.2488, -0.1124, -0.1502, -0.0235,  0.2322,  0.0468,\n",
       "                       -0.1220, -0.0146,  0.0174, -0.1907,  0.0216, -0.1558,  0.1970,  0.1340,\n",
       "                       -0.0797,  0.0195,  0.0799, -0.1061, -0.0675, -0.1250,  0.1405,  0.2641,\n",
       "                        0.2905, -0.0048,  0.2418,  0.0194,  0.0602, -0.0996,  0.1251,  0.0609,\n",
       "                        0.1672, -0.3125,  0.1441, -0.1875, -0.1528, -0.0278,  0.1001,  0.0575,\n",
       "                       -0.0173, -0.0532, -0.0284, -0.0805,  0.0099,  0.0263, -0.1142, -0.0750,\n",
       "                        0.0274, -0.0336, -0.2842,  0.0752, -0.2158, -0.1066, -0.0840, -0.0951,\n",
       "                        0.1162,  0.0771,  0.3536,  0.0541, -0.0369, -0.2596,  0.0762, -0.2456,\n",
       "                       -0.0170,  0.1241,  0.0647,  0.0903,  0.1528, -0.0184,  0.4968, -0.0164,\n",
       "                       -0.1031, -0.2541,  0.0582,  0.1493,  0.0402, -0.0414,  0.2170,  0.1468,\n",
       "                        0.1868,  0.0973, -0.1105, -0.0793,  0.0507,  0.1197,  0.2293, -0.0673,\n",
       "                        0.0844,  0.3093,  0.0897, -0.0107, -0.0570,  0.0461,  0.0891,  0.0457,\n",
       "                        0.2915,  0.1919, -0.1843,  0.0597, -0.0849, -0.0218,  0.1018, -0.1464,\n",
       "                        0.1401,  0.0810,  0.2079,  0.0940, -0.0690, -0.0726,  0.2977,  0.1968,\n",
       "                        0.0588,  0.0850,  0.1329,  0.0136,  0.0780, -0.0796,  0.2062,  0.0113,\n",
       "                       -0.0476,  0.1833,  0.2077,  0.0474,  0.1269,  0.1175, -0.0467, -0.1859])),\n",
       "              ('lstm.weight_ih_l0_reverse',\n",
       "               tensor([[-0.1676,  0.0163,  0.1923,  ..., -0.1645, -0.0809, -0.0528],\n",
       "                       [-0.1157,  0.1735,  0.0631,  ...,  0.1118, -0.0970, -0.0222],\n",
       "                       [-0.1131,  0.1328,  0.2441,  ...,  0.2050,  0.0136,  0.0229],\n",
       "                       ...,\n",
       "                       [-0.0173,  0.1098,  0.2101,  ...,  0.1130,  0.0160, -0.1360],\n",
       "                       [ 0.0596,  0.0045,  0.0488,  ...,  0.1333,  0.0970, -0.1510],\n",
       "                       [ 0.0376, -0.0611,  0.1079,  ..., -0.0339, -0.0308, -0.0881]])),\n",
       "              ('lstm.weight_hh_l0_reverse',\n",
       "               tensor([[-1.0957e-01, -5.8764e-02, -1.1990e-01,  ..., -4.0832e-03,\n",
       "                        -2.4865e-03,  1.1201e-04],\n",
       "                       [ 6.4195e-02, -3.9588e-03,  6.5674e-02,  ...,  3.1197e-02,\n",
       "                        -9.3259e-02,  3.9444e-02],\n",
       "                       [-9.4026e-02, -5.9244e-02, -5.4182e-02,  ..., -1.2409e-01,\n",
       "                        -7.9232e-02,  8.3847e-02],\n",
       "                       ...,\n",
       "                       [-5.7422e-03,  9.9084e-02, -9.9862e-03,  ..., -1.0474e-01,\n",
       "                        -4.3508e-02, -9.7674e-02],\n",
       "                       [-9.2821e-02, -3.7532e-02,  1.0775e-01,  ..., -4.1990e-02,\n",
       "                        -1.1795e-01, -9.5777e-02],\n",
       "                       [ 9.2462e-02,  1.3895e-03,  8.7567e-02,  ...,  1.9280e-02,\n",
       "                         4.6498e-02, -4.3757e-03]])),\n",
       "              ('lstm.bias_ih_l0_reverse',\n",
       "               tensor([-8.2647e-02, -2.3165e-01, -2.4086e-01,  2.8627e-02,  2.5239e-01,\n",
       "                        2.6249e-02,  1.1955e-01, -1.4247e-01, -5.0180e-02, -1.5993e-01,\n",
       "                       -2.1758e-02, -5.9003e-02,  4.4232e-02,  1.2965e-02, -1.4714e-01,\n",
       "                       -7.8405e-02, -1.3426e-01, -1.7046e-01, -4.7758e-02, -1.0065e-01,\n",
       "                       -1.0751e-01,  4.9663e-02, -1.5960e-01, -5.5563e-02, -9.2066e-02,\n",
       "                       -6.5564e-02,  6.1994e-03, -1.2116e-01, -7.3204e-02, -1.9435e-01,\n",
       "                        1.0658e-02, -1.3573e-01, -2.4386e-01, -1.0790e-01, -1.3286e-02,\n",
       "                        2.7553e-01, -1.7646e-01, -1.8391e-01, -2.2862e-01,  6.7950e-03,\n",
       "                       -1.5993e-01, -1.4794e-01, -8.6435e-02, -9.3795e-02, -1.7464e-01,\n",
       "                       -1.6293e-01,  8.5447e-03,  1.0056e-01,  1.0044e-01,  1.9124e-02,\n",
       "                       -2.1365e-02, -1.1306e-01, -1.8005e-01, -1.0684e-02, -1.2459e-01,\n",
       "                        1.2072e-02, -1.3863e-01, -5.2087e-02, -4.3922e-02, -1.8820e-02,\n",
       "                        1.6996e-02, -1.8356e-01, -7.1339e-02,  3.0652e-02, -3.3342e-02,\n",
       "                       -9.6938e-02,  2.7917e-03,  4.8840e-02, -6.5420e-02,  5.9986e-02,\n",
       "                       -1.0869e-01, -1.1120e-01, -7.5838e-02,  9.4433e-02, -7.9600e-02,\n",
       "                        1.2312e-01, -7.0333e-02, -8.6470e-02,  9.7537e-02, -1.0598e-01,\n",
       "                       -4.8146e-02, -8.3335e-03,  1.0545e-01, -3.0794e-03, -8.7349e-02,\n",
       "                       -3.3486e-02,  9.5329e-02, -1.2185e-01, -5.8820e-02, -9.4157e-02,\n",
       "                       -1.3129e-02,  7.7830e-02,  9.0447e-02,  1.6135e-02, -6.6732e-02,\n",
       "                       -6.5654e-02, -1.2326e-01, -5.9263e-02,  1.0613e-01,  1.0269e-02,\n",
       "                        2.3609e-02, -1.0317e-01, -1.0852e-01,  4.2434e-02, -1.1365e-01,\n",
       "                        8.0723e-02,  1.0615e-01, -1.5366e-02,  3.9003e-02, -8.6943e-02,\n",
       "                       -1.9009e-02, -4.5207e-02, -5.2641e-02, -7.5380e-02,  4.5172e-03,\n",
       "                       -1.3773e-02, -9.1855e-02,  4.0437e-02, -2.9924e-02,  3.7005e-02,\n",
       "                       -6.1353e-03,  9.3346e-03,  6.6810e-02,  1.1606e-01,  9.2403e-03,\n",
       "                       -9.0696e-02,  2.3832e-02, -1.2205e-01, -4.6427e-03, -1.3004e-01,\n",
       "                        1.7401e-02,  3.6096e-03, -3.8636e-02, -3.4087e-02, -3.4366e-02,\n",
       "                        1.6324e-01, -9.2908e-02,  1.2547e-01,  7.2256e-02,  1.5174e-01,\n",
       "                        2.4285e-01, -1.9837e-02, -3.4560e-02, -3.1263e-02, -8.9006e-02,\n",
       "                       -4.7824e-02,  1.1929e-01,  1.0515e-02,  1.1995e-01,  5.6546e-02,\n",
       "                        1.0608e-01,  4.8141e-02,  1.0103e-01, -3.4752e-02,  4.4984e-02,\n",
       "                        4.5773e-02,  1.2478e-01, -7.4704e-02,  7.8801e-02,  5.1531e-02,\n",
       "                        1.4158e-01, -7.7641e-02, -6.9350e-02,  4.2529e-03,  4.1962e-02,\n",
       "                        7.6809e-02, -1.7055e-02, -6.6169e-02,  5.7608e-02, -8.1028e-02,\n",
       "                       -3.3378e-02, -6.4648e-02, -7.0297e-02, -4.3077e-02, -4.8329e-02,\n",
       "                        1.7338e-01,  4.7401e-02,  3.4988e-02,  9.7954e-02, -5.8980e-02,\n",
       "                        1.0909e-02, -9.5031e-02, -4.5312e-02, -1.7504e-01, -2.0505e-02,\n",
       "                        2.8307e-02, -2.0645e-02,  1.3563e-01, -6.7382e-02,  1.2178e-01,\n",
       "                       -5.5356e-02,  8.6499e-02, -1.3678e-01, -1.1137e-01, -2.1778e-01,\n",
       "                       -9.0219e-03,  8.4148e-02,  4.2429e-02,  4.6166e-02, -7.2931e-02,\n",
       "                       -1.7846e-01, -1.4268e-01,  1.0414e-02, -1.4968e-01,  2.3071e-01,\n",
       "                       -6.0015e-02, -2.2417e-01, -3.9630e-02, -6.7815e-02, -2.7960e-02,\n",
       "                       -5.8816e-02, -1.9347e-01, -1.0040e-02, -2.5570e-06, -1.4657e-01,\n",
       "                       -2.2701e-01, -1.1019e-02, -1.3438e-01,  4.0575e-02, -1.3287e-01,\n",
       "                       -1.4672e-01,  3.6674e-02, -1.1470e-01, -1.4291e-01, -1.2210e-01,\n",
       "                       -9.8023e-02, -4.3020e-02,  1.0982e-01, -9.2471e-02, -2.5821e-02,\n",
       "                        2.0049e-02, -6.3900e-02, -2.1125e-01, -1.2674e-01, -7.0266e-02,\n",
       "                       -1.3979e-01, -2.6715e-02, -1.3147e-01,  1.0269e-01,  2.4064e-01,\n",
       "                        7.3701e-02,  1.3005e-02, -4.6914e-02, -5.1722e-02, -2.1104e-01,\n",
       "                        8.9508e-03, -1.3348e-01,  1.6382e-02, -1.0552e-01, -1.3770e-01,\n",
       "                       -1.7649e-02, -3.2550e-02, -1.3929e-01, -3.1649e-02, -1.3435e-01,\n",
       "                       -9.8447e-02])),\n",
       "              ('lstm.bias_hh_l0_reverse',\n",
       "               tensor([-1.4823e-01, -2.1456e-01, -1.2267e-01,  1.8985e-02,  6.9329e-02,\n",
       "                       -1.7402e-01, -2.9625e-02, -2.3891e-01, -6.6609e-02, -1.5998e-01,\n",
       "                       -1.8158e-01, -5.0125e-02,  1.8218e-01, -1.4859e-01, -6.4013e-02,\n",
       "                        3.4403e-02, -3.7985e-02, -8.5004e-02, -1.7894e-01, -2.4341e-01,\n",
       "                        1.9856e-02,  5.3236e-02, -7.0264e-02, -1.9096e-01,  1.1433e-03,\n",
       "                       -1.5380e-01, -1.4311e-01, -8.8380e-03, -2.1955e-02, -3.7470e-02,\n",
       "                       -4.2984e-02, -1.9648e-01, -1.7417e-01, -8.6605e-02,  9.1173e-02,\n",
       "                        1.6027e-01, -1.5355e-02, -1.4093e-01, -2.1087e-01, -3.6405e-02,\n",
       "                       -7.3366e-02, -1.3960e-01,  1.0089e-02, -7.0404e-02, -6.1768e-02,\n",
       "                       -6.6259e-02,  7.5543e-02,  1.7591e-01,  1.5641e-01, -1.5664e-01,\n",
       "                       -5.5253e-02, -1.4226e-01, -1.5965e-01, -1.9807e-01, -8.4967e-02,\n",
       "                       -3.6596e-02, -1.7465e-01, -9.5970e-02, -2.2442e-01, -9.7376e-02,\n",
       "                       -1.3995e-01, -2.1955e-01, -9.4221e-02,  4.3103e-02, -1.1023e-01,\n",
       "                        3.7364e-03,  1.8107e-02,  1.0391e-01, -8.5297e-02, -7.7380e-02,\n",
       "                        8.8604e-02, -9.8535e-02, -9.8219e-02,  6.0375e-02, -4.9068e-02,\n",
       "                        7.4238e-02,  9.6987e-04,  2.0797e-02, -6.0740e-02,  7.5475e-02,\n",
       "                       -3.3710e-02,  4.5161e-02,  2.1026e-02,  8.3229e-02, -7.2273e-02,\n",
       "                        1.7842e-02, -6.5487e-02,  1.2169e-01, -1.0811e-01, -3.8764e-02,\n",
       "                       -7.1093e-02,  1.1700e-01, -8.7530e-02, -1.0501e-01,  8.7720e-02,\n",
       "                        1.6841e-02, -8.2403e-02,  6.9143e-02, -9.9341e-02,  1.2316e-04,\n",
       "                       -1.0022e-01, -1.2029e-02, -8.9145e-02, -1.0290e-01,  4.4250e-02,\n",
       "                        4.0558e-02, -7.9060e-02,  3.7150e-02,  4.0593e-02,  1.9510e-02,\n",
       "                       -2.3587e-03,  6.7834e-02,  9.6924e-02, -1.1396e-01,  3.2193e-03,\n",
       "                        1.0579e-01, -4.7584e-02,  6.5579e-02,  7.9369e-02, -8.9804e-02,\n",
       "                       -2.4958e-02,  1.1657e-01,  1.0283e-01,  1.2181e-01, -8.5974e-02,\n",
       "                        5.3717e-02,  7.4294e-03,  1.5991e-02,  9.7838e-02,  9.4449e-02,\n",
       "                       -7.9636e-02, -9.4218e-02, -1.1663e-01,  6.9270e-02, -1.3332e-01,\n",
       "                       -7.7057e-03,  7.0671e-02,  7.2954e-02,  1.1162e-01,  1.2720e-01,\n",
       "                        7.2312e-02, -3.6846e-02, -1.7501e-02, -1.4214e-01,  8.5150e-02,\n",
       "                        7.9427e-02, -2.1703e-02, -1.1585e-01, -1.7076e-02,  2.9341e-02,\n",
       "                        2.7047e-02, -5.4950e-02,  4.1041e-02, -1.2247e-01,  3.7538e-03,\n",
       "                       -6.1030e-02,  9.7216e-02,  3.0454e-02,  3.7009e-02,  8.2477e-02,\n",
       "                        6.5095e-02,  6.6944e-02,  1.3221e-01,  1.9481e-02, -7.7904e-02,\n",
       "                       -1.4048e-01, -3.3626e-02,  1.2251e-02,  6.0833e-02,  2.6264e-02,\n",
       "                       -1.8752e-02, -7.7632e-02,  8.7988e-02,  1.1359e-01,  1.5441e-01,\n",
       "                        1.4416e-01,  2.0902e-02, -2.0630e-02, -3.3719e-02,  7.6647e-02,\n",
       "                        8.4367e-03,  5.5842e-02,  1.3413e-01,  4.9319e-03, -7.8393e-02,\n",
       "                        5.7099e-02,  5.7679e-02,  4.2878e-02, -7.4116e-03,  7.7293e-02,\n",
       "                       -6.2030e-02,  6.6831e-02,  1.9132e-02, -9.5285e-02, -1.0043e-02,\n",
       "                       -1.6970e-01,  2.3047e-01,  3.6362e-02,  7.7262e-02, -1.7797e-01,\n",
       "                        2.0549e-04, -1.7809e-01, -9.1800e-02,  4.7140e-02,  4.5076e-02,\n",
       "                       -2.0863e-02, -1.9779e-01, -9.9409e-02, -1.3465e-01,  3.2735e-02,\n",
       "                       -1.2384e-01, -8.6427e-02, -1.5298e-01,  6.0501e-02, -1.1445e-01,\n",
       "                       -2.2334e-01, -1.2753e-01,  3.2083e-02,  3.4644e-02, -5.5066e-02,\n",
       "                       -1.9219e-01, -1.2197e-01, -8.5887e-02, -2.2114e-01, -1.8213e-01,\n",
       "                       -1.0645e-01,  7.3165e-02,  9.8523e-02, -1.6356e-01, -1.4352e-01,\n",
       "                       -1.7181e-01, -6.4521e-02, -2.1367e-01,  2.3859e-03, -1.6853e-01,\n",
       "                       -3.2275e-02, -4.0160e-02, -1.7502e-01,  2.0338e-02,  6.2268e-02,\n",
       "                        1.3435e-01, -2.1597e-02,  9.3533e-03, -8.4617e-02, -7.7327e-03,\n",
       "                       -1.0378e-01, -1.0063e-01,  2.7690e-02, -1.7384e-01, -1.6950e-01,\n",
       "                       -6.0090e-02, -1.2329e-01,  3.4210e-03, -4.3754e-02,  1.3823e-02,\n",
       "                       -1.2200e-01])),\n",
       "              ('fc1.weight',\n",
       "               tensor([[ 0.0513, -0.1663,  0.0368,  ..., -0.0423,  0.0330, -0.0894],\n",
       "                       [-0.0769,  0.1521,  0.0182,  ...,  0.0087,  0.0432,  0.1406],\n",
       "                       [ 0.0199,  0.0151, -0.0078,  ...,  0.0366,  0.0232, -0.0331],\n",
       "                       ...,\n",
       "                       [-0.0956, -0.0611, -0.0023,  ...,  0.0416, -0.0209,  0.0728],\n",
       "                       [-0.2084,  0.1991,  0.0017,  ..., -0.0543, -0.0649,  0.1664],\n",
       "                       [-0.0379,  0.0244, -0.0487,  ...,  0.0411,  0.0359, -0.0241]])),\n",
       "              ('fc1.bias',\n",
       "               tensor([ 0.0779,  0.0560, -0.0786, -0.1236, -0.0317,  0.0512,  0.0677,  0.0731,\n",
       "                       -0.0010, -0.1140,  0.0592,  0.1034,  0.0004, -0.0301, -0.0078, -0.0581,\n",
       "                       -0.0808, -0.0201,  0.0510, -0.0827,  0.0655, -0.0201,  0.1570, -0.0548,\n",
       "                       -0.0194, -0.0330,  0.0879, -0.0452,  0.0116, -0.0942,  0.1275, -0.0039,\n",
       "                       -0.0779, -0.0782,  0.0098,  0.0394, -0.0674, -0.0428,  0.0631, -0.0281,\n",
       "                       -0.0096, -0.0667,  0.0819,  0.0587,  0.1173, -0.0042,  0.1035,  0.1537,\n",
       "                       -0.0703, -0.1248,  0.0466,  0.0103,  0.0034,  0.0122, -0.0884, -0.0764,\n",
       "                       -0.0298, -0.0827,  0.0334,  0.0714,  0.1219,  0.0822, -0.1053,  0.0547])),\n",
       "              ('fc2.weight',\n",
       "               tensor([[-1.0890e-01,  1.7618e-01, -9.3564e-02,  6.0200e-02,  2.0210e-01,\n",
       "                        -2.7219e-01,  1.2763e-01, -6.0744e-02,  5.8139e-03, -1.8689e-03,\n",
       "                         4.6368e-02,  1.9937e-01,  1.2080e-01,  3.1550e-02,  9.3513e-02,\n",
       "                        -5.3994e-02, -7.0514e-02,  7.0257e-02, -7.0237e-03,  1.6089e-01,\n",
       "                         1.9414e-01,  1.3926e-01,  1.3588e-01,  1.5594e-01, -2.3190e-02,\n",
       "                        -5.3144e-02, -9.4220e-02,  2.2714e-03, -4.6239e-02, -5.4798e-02,\n",
       "                        -9.5662e-02,  1.3849e-01, -1.1003e-01,  7.3116e-03, -8.2959e-02,\n",
       "                         5.9771e-02, -3.4652e-02,  1.7519e-01,  9.5165e-02, -9.1706e-02,\n",
       "                        -8.1999e-02, -5.6024e-02,  8.9709e-02,  2.1715e-01, -2.2696e-01,\n",
       "                         1.6690e-01, -1.1193e-01, -9.1475e-03, -1.0457e-01,  4.5339e-03,\n",
       "                         5.8494e-02, -5.2785e-02, -6.9754e-02, -7.8884e-02,  6.8649e-03,\n",
       "                         2.1266e-02, -3.4039e-02, -3.2987e-02,  1.5485e-01,  7.9951e-02,\n",
       "                         1.3890e-01, -1.3813e-01,  1.2804e-01, -1.3146e-01],\n",
       "                       [ 1.1236e-01, -4.3997e-01, -4.1693e-02, -8.6200e-03, -1.0518e-01,\n",
       "                        -6.4972e-02,  1.9025e-01,  1.1065e-01, -8.5921e-02,  9.8745e-03,\n",
       "                         1.4332e-01, -7.1457e-01,  1.5006e-01,  6.3760e-02,  7.1890e-02,\n",
       "                        -1.4080e-01, -1.6253e-01,  8.6689e-04, -1.4144e-02, -3.2328e-01,\n",
       "                        -6.4031e-01,  1.2427e-01,  1.2153e-01, -6.3308e-01, -2.2131e-01,\n",
       "                        -6.6082e-02,  7.2184e-02,  2.7592e-02, -4.8106e-02, -1.2476e-01,\n",
       "                        -8.2493e-02, -5.8033e-01,  6.0826e-02,  2.3176e-02, -1.6093e-01,\n",
       "                         4.4065e-02, -7.8139e-02,  1.5628e-01,  1.1630e-01, -4.9389e-02,\n",
       "                         2.9593e-02,  7.9083e-02,  1.4477e-01, -1.7602e-01, -2.9194e-01,\n",
       "                         1.8977e-01,  1.2956e-01,  1.0033e-01, -4.4847e-03, -8.4554e-02,\n",
       "                         8.3508e-02,  7.9414e-02, -1.1216e-03, -2.0068e-01, -9.0696e-02,\n",
       "                         2.0164e-02,  2.2701e-03, -1.3006e-01, -5.7755e-01,  1.8386e-01,\n",
       "                         2.0291e-01,  1.1018e-01, -5.8765e-01, -7.0083e-02],\n",
       "                       [ 1.6891e-01, -1.0016e-01, -1.0034e-01, -6.3219e-02, -1.3511e-01,\n",
       "                         2.2872e-01, -6.8194e-01,  1.6785e-01, -6.3643e-02, -5.6297e-02,\n",
       "                         1.5327e-01, -1.8067e-01, -6.4381e-01, -2.0837e-03, -6.2445e-02,\n",
       "                         1.0879e-01, -2.9501e-02, -7.4033e-02,  1.9069e-01,  4.9566e-02,\n",
       "                        -7.9643e-02, -5.1042e-01, -5.5148e-01,  2.0979e-02,  8.4782e-02,\n",
       "                        -6.8274e-03,  1.7236e-01,  7.0059e-02,  2.0469e-01,  9.4684e-02,\n",
       "                         1.6358e-01, -3.0412e-01,  1.6797e-01,  3.1630e-02,  2.2973e-01,\n",
       "                         2.4097e-02, -3.5282e-02, -3.0639e-01, -6.1321e-01,  1.4127e-03,\n",
       "                        -4.6604e-02,  3.7874e-02, -7.4899e-01, -4.3273e-02,  1.8653e-01,\n",
       "                        -4.3925e-01,  1.6103e-01,  1.6781e-01,  1.3305e-01,  5.3766e-02,\n",
       "                        -3.6717e-02,  2.2055e-01,  4.7114e-03,  1.3342e-01,  7.6546e-02,\n",
       "                        -1.0620e-01, -4.8215e-02,  2.8751e-02, -1.9711e-01, -5.6140e-01,\n",
       "                        -4.3433e-01,  1.7327e-01, -1.5014e-01, -2.2564e-02],\n",
       "                       [ 4.3092e-02, -3.0453e-02,  1.1067e-01,  1.1624e-01,  5.4485e-02,\n",
       "                        -1.3193e-01, -4.2868e-02, -6.7073e-02,  1.2236e-01, -1.1045e-01,\n",
       "                        -8.7872e-02,  1.4504e-02, -7.1405e-02, -1.6640e-01, -2.0481e-02,\n",
       "                        -4.2383e-02,  3.9994e-02,  9.7975e-02, -1.4912e-01,  2.7323e-02,\n",
       "                        -1.7216e-02, -8.7230e-02, -1.0460e-01,  1.0496e-01, -1.0234e-01,\n",
       "                         9.0945e-02, -1.0079e-01, -5.9259e-02, -4.8680e-02,  2.8886e-03,\n",
       "                        -4.2446e-02, -8.1277e-02, -1.8635e-02, -4.5783e-02,  8.4237e-03,\n",
       "                         1.0063e-01,  6.8922e-02, -1.2191e-01, -8.6767e-02,  7.9469e-03,\n",
       "                        -5.6969e-02, -4.5072e-02, -3.5655e-02,  2.2907e-02, -3.0984e-02,\n",
       "                        -1.4251e-01,  1.5027e-02,  2.1845e-02,  1.2397e-02, -1.0471e-01,\n",
       "                         8.7524e-02, -7.4822e-02, -9.4713e-02,  8.5992e-02,  4.0877e-02,\n",
       "                         3.3929e-03, -1.0215e-01,  1.1474e-01, -7.3105e-02,  1.1930e-02,\n",
       "                        -2.9652e-02, -9.6781e-02, -6.5626e-02,  2.1566e-02],\n",
       "                       [-4.1386e-02, -4.5290e-02, -3.5838e-02,  9.5937e-02, -1.3735e-01,\n",
       "                        -2.9813e-02, -9.7215e-02, -5.9646e-02, -8.4306e-02,  4.3927e-02,\n",
       "                        -1.1922e-01, -6.2560e-02, -3.4409e-02, -5.7094e-02,  7.5386e-02,\n",
       "                        -6.7512e-02,  8.1517e-02,  3.3671e-03,  1.8687e-02, -6.0078e-02,\n",
       "                        -4.3642e-02, -3.6178e-03, -1.0178e-01, -1.2494e-01, -8.5677e-02,\n",
       "                         8.6839e-02, -1.1877e-01, -5.7650e-03, -1.9163e-02, -6.4059e-02,\n",
       "                         7.0107e-02,  3.0414e-02, -3.2522e-02,  1.1000e-01, -9.8566e-02,\n",
       "                         2.7327e-02, -9.4608e-02, -7.2910e-02, -1.0349e-01,  6.6853e-02,\n",
       "                         1.9852e-02, -7.8031e-02, -6.3115e-02,  4.0583e-02, -4.2914e-03,\n",
       "                         3.3915e-03, -3.3151e-02, -3.1759e-04, -1.3241e-02, -1.0886e-01,\n",
       "                        -2.3747e-02, -3.0085e-02,  8.9186e-02, -6.9454e-02, -1.1608e-02,\n",
       "                        -1.1075e-01, -6.8848e-03, -4.6264e-02, -1.1181e-01, -1.5696e-01,\n",
       "                        -1.0520e-01, -7.5662e-02, -1.4232e-02,  5.1180e-02],\n",
       "                       [-6.4689e-03, -7.1751e-02,  8.8526e-02,  5.8601e-02,  1.8085e-02,\n",
       "                        -6.1457e-02, -3.5091e-02, -8.5633e-02,  1.0157e-02, -8.1924e-02,\n",
       "                        -1.6899e-01, -3.3068e-02, -9.9326e-02,  8.7559e-02,  9.5060e-02,\n",
       "                        -3.4414e-02, -1.3697e-01,  2.0138e-02, -8.2134e-02, -2.5015e-03,\n",
       "                         9.0137e-03, -1.6697e-01, -8.8358e-02,  1.8976e-02,  8.6017e-03,\n",
       "                        -8.6794e-02, -1.4393e-01, -9.8108e-02,  3.6356e-02, -1.2499e-01,\n",
       "                         2.7568e-02,  2.6614e-02, -1.1051e-01,  2.1813e-02, -7.5112e-03,\n",
       "                        -5.4178e-02, -8.7276e-02, -8.7376e-02, -9.3058e-02,  2.1485e-02,\n",
       "                        -7.2300e-02, -7.2677e-02, -8.5941e-02, -5.2381e-02, -1.2252e-02,\n",
       "                        -1.6598e-01, -1.8940e-01, -1.6090e-01,  2.5341e-02, -5.5919e-02,\n",
       "                         7.9118e-04, -1.6620e-01,  4.4435e-02,  4.2585e-02,  8.7147e-02,\n",
       "                         5.7505e-02,  5.0447e-02,  6.0762e-02, -1.2347e-01, -9.1328e-02,\n",
       "                        -6.3861e-02, -2.8016e-02,  5.1640e-02,  3.4385e-02],\n",
       "                       [-8.4990e-02, -6.0152e-02,  1.0812e-01,  9.0903e-02, -1.0701e-01,\n",
       "                        -1.4877e-01,  2.0833e-02, -1.3338e-01,  8.8966e-02,  7.5242e-03,\n",
       "                        -1.4195e-01, -4.8610e-02, -1.0359e-01,  1.0281e-01,  1.1594e-02,\n",
       "                         5.0044e-02, -1.2684e-01,  5.1842e-02, -1.0541e-01,  6.8274e-02,\n",
       "                        -7.0408e-02, -4.0351e-02, -6.1528e-02, -4.3931e-02,  1.7154e-02,\n",
       "                         1.1360e-01, -1.1824e-01,  9.4426e-02, -6.9163e-03, -6.0657e-02,\n",
       "                        -8.0059e-02,  6.0843e-02, -1.1795e-01,  4.1692e-02,  5.1410e-02,\n",
       "                         8.7633e-02, -2.4166e-02, -7.8986e-02, -8.1131e-02,  2.9283e-02,\n",
       "                        -7.7750e-02, -3.4791e-02, -1.0788e-01,  7.0718e-03,  8.2539e-02,\n",
       "                        -7.9776e-03, -7.1029e-02, -7.9968e-02, -9.4531e-02,  7.7650e-02,\n",
       "                         1.5328e-02, -1.1580e-02,  7.5541e-02, -6.7361e-02, -1.0153e-01,\n",
       "                         5.7027e-02,  2.2423e-02,  1.5256e-02, -2.0236e-02, -1.0048e-01,\n",
       "                        -8.1799e-02,  1.0583e-03, -1.3183e-02, -9.0876e-02],\n",
       "                       [-7.2986e-02, -9.9480e-02,  1.0017e-01,  1.1070e-01,  1.1748e-01,\n",
       "                        -7.9178e-02, -1.0254e-01, -1.1020e-01,  1.1012e-01, -1.1651e-01,\n",
       "                        -9.1438e-02, -3.8330e-02, -1.3200e-01,  4.0149e-02,  2.8602e-02,\n",
       "                         5.9655e-02, -5.8248e-02,  1.1247e-01, -3.3092e-02, -1.4594e-01,\n",
       "                        -1.5772e-02, -7.6375e-02, -4.0430e-02,  1.5634e-03,  8.4032e-02,\n",
       "                         5.4504e-02, -2.7262e-02,  3.1871e-02,  1.1532e-01, -1.2261e-02,\n",
       "                         2.1457e-02, -8.7996e-02, -7.4833e-03,  1.0917e-01, -1.5066e-02,\n",
       "                         4.9102e-02,  7.0673e-02, -1.3669e-01, -8.4876e-02,  7.5711e-02,\n",
       "                         7.7575e-02, -1.0560e-01, -4.3286e-02,  1.0787e-01, -4.4521e-02,\n",
       "                        -4.2626e-02, -1.2614e-01, -1.1582e-01, -8.2904e-02,  1.5051e-02,\n",
       "                        -7.5516e-02, -9.2249e-02, -1.1287e-01, -1.0343e-01,  1.0505e-01,\n",
       "                        -4.7283e-02,  5.7639e-02,  7.0723e-02, -2.7328e-02, -1.1790e-02,\n",
       "                        -2.3304e-02, -1.2280e-01,  3.5118e-02,  1.2666e-02]])),\n",
       "              ('fc2.bias',\n",
       "               tensor([ 0.5972,  0.5112,  0.9745, -0.1897, -0.1167, -0.0597, -0.0329, -0.0272])),\n",
       "              ('fc3.weight',\n",
       "               tensor([[-0.0660, -0.2683, -0.3143, -0.1471, -0.0652, -0.2006, -0.0903,  0.0956],\n",
       "                       [-0.0408,  0.2104, -0.0739, -0.1498,  0.0518,  0.0933, -0.1356, -0.0811],\n",
       "                       [-0.1917, -0.2377, -0.0293,  0.0899,  0.0733,  0.0771,  0.1045, -0.1211]])),\n",
       "              ('fc3.bias', tensor([ 0.0548, -0.2347, -0.2780]))]),\n",
       " 0.7806666666666666)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMModel(vocab_size, embedding_dim, hidden_size, output_size, max_length).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
